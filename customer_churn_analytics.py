# -*- coding: utf-8 -*-
"""Customer Churn Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zN2QmzNBvc8xfUr-vGLLrYl3Ls3-Z8WC

The source of the dataset used is from Kaggle (https://www.kaggle.com/kmalit/bank-customer-churn-prediction). The software used in developing this system is Google Colab.
"""

from google.colab import drive
drive.mount('/content/drive')

"""##Import Libraries and Data Preparation##"""

# Commented out IPython magic to ensure Python compatibility.
# Required libraries
# For data wrangling
import pandas as pd
import numpy as np

# Read the DataFrame
bank = pd.read_csv('/content/drive/MyDrive/Churn_Modelling.csv')

# For visualization
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
pd.options.display.max_rows = None
pd.options.display.max_columns = None
from sklearn.model_selection import train_test_split

# Standardize data value
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Imbalance data
from imblearn.over_sampling import SMOTE

# Remove warning messages
import warnings
warnings.filterwarnings("ignore")

# Scoring functions
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score, recall_score

"""## **B. Data Understanding**##
The churn analytics system adopted by Autumn bank load the datasets which contains the customers' information in order to analyse the customers information and predict the customer churn rate. The details about the datasets are shown in below.
"""

#Review the top rows of what os left of the dataframe
bank.head()

#check data types of columns
dataTypeSeries = bank.dtypes
dataTypeSeries

bank.info()

#How much data is contained in this dataset?
datasetSize = bank.shape
datasetSize
#In this dataset it contain 10000 rows and 14 columns

#Checking the dataset have missing values or not
nullValue = bank.isnull().any()
nullValue

"""###Data Visualization / Exploratory Data Analysis(EDA)###"""

#display the chart
sns.countplot(y="Exited", data=bank)
plt.show()

#No of exited vs Active  get the percentage split figure
labels = 'Exited', 'Retained'
sizes = [bank.Exited[bank['Exited']==1].count(), bank.Exited[bank['Exited']==0].count()]
explode = (0, 0.1)
fig1, ax1 = plt.subplots(figsize=(10, 8))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')
plt.title("Proportion of customer churned and retained", size = 20)
plt.show()

# We first review the 'Status' relation with categorical variables
_, ax = plt.subplots(1, 3, figsize=(18, 6))
plt.subplots_adjust(wspace=0.3)
sns.countplot(x = "NumOfProducts", hue="Exited", data = bank, ax= ax[0])
sns.countplot(x = "HasCrCard", hue="Exited", data = bank, ax = ax[1])
sns.countplot(x = "IsActiveMember", hue="Exited", data = bank, ax = ax[2])

bank["Age"].value_counts().plot.bar(figsize=(20,6))

facet = sns.FacetGrid(bank, hue="Exited",aspect=3)
facet.map(sns.kdeplot,"Age",shade= True)
facet.set(xlim=(0, bank["Age"].max()))
facet.add_legend()

plt.show()

_, ax =  plt.subplots(1, 2, figsize=(15, 7))
cmap = sns.cubehelix_palette(light=1, as_cmap=True)
sns.scatterplot(x = "Age", y = "Balance", hue = "Exited", cmap = cmap, sizes = (10, 200), data = bank, ax=ax[0])
sns.scatterplot(x = "Age", y = "CreditScore", hue = "Exited", cmap = cmap, sizes = (10, 200), data = bank, ax=ax[1])

facet = sns.FacetGrid(bank, hue="Exited",aspect=3)
facet.map(sns.kdeplot,"Balance",shade= True)
facet.set(xlim=(0, bank["Balance"].max()))
facet.add_legend()

plt.show()

_, ax = plt.subplots(1, 2, figsize=(15, 6))
sns.scatterplot(x = "Balance", y = "Age", data = bank, hue="Exited", ax = ax[0])
sns.scatterplot(x = "Balance", y = "CreditScore", data = bank, hue="Exited", ax = ax[1])

facet = sns.FacetGrid(bank, hue="Exited",aspect=3)
facet.map(sns.kdeplot,"CreditScore",shade= True)
facet.set(xlim=(0, bank["CreditScore"].max()))
facet.add_legend()

plt.show()

"""### Checking Correlation"""

plt.subplots(figsize=(11,8))
sns.heatmap(bank.corr(), annot=True, cmap="RdYlBu")
plt.show()

"""## **C. Data Preparation**##


### Data Cleaning###
There are many variables contained in the dataset but some variables are unnecessary or unneeded for predicting the churn analysis. In order to avoid getting erroneous that will affect the results outcome, the data cleaning is being carried out because it is an important process before doing prediction churn analysis. From the dataset we selected, the columns which are unnecessary for the churn analytics namely ‘RowNumber’, ‘CustomerId’ and ‘Surname’ are being removed.
"""

bank = bank.drop(['RowNumber','Surname','CustomerId'], axis=1)

bank.head()

"""###Transform Categorical to Numeric###
Next, there are many machine learning algorithms that can support the categorical values without further manipulation but still have some algorithms that do not accept the categorical data. From the dataset, we can see that most of the columns data are present in numeric only except the ‘Geography’ and ‘Gender’ which are present in categorial features. Thus, we apply the pandas and scikit-learn to transform the categorical data which are ‘Geography’ and ‘Gender’ into suitable numeric values. Below are the codes and output after transforming.
"""

encoder = LabelEncoder()
bank["Geography"] = encoder.fit_transform(bank["Geography"])
bank["Gender"] = encoder.fit_transform(bank["Gender"])

bank.head()

"""###Data Preprocessing###

### Handle Missing Value
In the data understanding stage, below shows that there are no missing values in the dataset. Due to no missing values included inside, we no longer need to handle the missing values and replace it.

###Check missing value###
"""

#check missing value
nullValue = bank.isnull().any()
nullValue

# Drops missing values for all columns
bank = bank.dropna(how='any')

"""### Handle Outlier###
We are checking all the attributes in the dataset that have any outlier contained. After we cheked, we found that 'CreditScore', 'Age', 'NumofProducts' and 'Exited' are containing outlier. Thus, we decided it which are a valid outlier or invalid outlier. In a result, we find that they are the valid outlier, so remain it.

###Checking Outlier###
"""

plt.subplots(figsize = (20,10))
bank.boxplot(patch_artist= True, sym="k.")

#format the figure size to 20rows,10cols
plt.subplots(figsize = (14,6))

#plot graph by Credit Score
plt.subplot(221)
sns.boxplot(x = bank["CreditScore"])
plt.title("Credit Score", fontsize = 20)

#plot graph by Age
plt.subplot(222)
sns.boxplot(x = bank["Age"])
plt.title("Age", fontsize = 20)

#plot graph by NumOfProducts
plt.subplot(223)
sns.boxplot(x = bank["NumOfProducts"])
plt.title("Num Of Products", fontsize = 20)

#plot graph by Exited
plt.subplot(224)
sns.boxplot(x = bank["Exited"])
plt.title("Exited", fontsize = 20)

plt.tight_layout()
plt.show()

#count the rows with "Y" and "N" in the Exited
bank['Exited'].value_counts()

#highly unbalanced dataset,
#lets describe data
bank.describe()

"""###Split Train & Test###"""

C = bank.drop("Exited", axis=1)

X = np.array(bank.iloc[:, bank.columns != 'Exited'])
y = np.array(bank.iloc[:, bank.columns == 'Exited'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print("X_train: ",X_train.shape)
print("y_train: ",y_train.shape)

print("X_test: ",X_test.shape)
print("y_test: ",y_test.shape)

"""###SMOTE###
As our dataset is imbalance, so we need to use SMOTE to handle the imbalance dataset before modelling it.

###Use SMOTE to balance the number of occurence of a label###
"""

print('Shape of X: {}'.format(X.shape))
print('Shape of y: {}'.format(y.shape))

print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

from imblearn.over_sampling import SMOTE

print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0)))

sm = SMOTE(random_state=0)
X_train, y_train = sm.fit_resample(X_train, y_train.ravel())

# Confirm the data has been balanced
print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train == 0)))

"""## **D. Modelling**##

###Algorithm Selected###
There are 9 algorithms applied to perform modelling on the dataset in order to

###1)Logistic Regression###
Logistic regression is a statistical model that in its basic form uses a logistic function to solve the binary classification problem. Logistic regression can be used for various classification problems such as will the customers churn or not. Logistic regression is able to generate results which is not complicated in a short time and it is easy to implement. Furthermore, logistic regresiion describes and estimated the relationship between one dependent binary variable and independent variables.

###2)Random Forest###
Random forest is a supervised learning algorithm which can be used for both classification and regression. It is an algorithm which is easy to implement and it can handle both missing values and outliers automatically. It creates decision tress on randomly selected data samples and generate prediction from each tree. So, by using the random forest algorithm, customer churn can be predicted and identified based on the results generated from the trees.



###3)XGBoost Classifier###
XGBoost as known as "Extreme Gradient Boosting". It is a decision-tree-based ensemble machine learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data such as image and text, artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured or tabular data, decision tree based algorithms are considered best-in-class right now.

###Logistic Regression###
"""

# Commented out IPython magic to ensure Python compatibility.
## Logistic Regression

#import the library
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

lr = LogisticRegression()

#Train the mdoel
lr.fit(X_train, y_train)

pred_lr_train = lr.predict(X_train)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_lr_train]))

#display confusion matrix for trainset
lr_cm_train = confusion_matrix(y_train,pred_lr_train)
sns.heatmap(lr_cm_train,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = lr_cm_train[0]
true_pos, false_neg = lr_cm_train[1]

#Take the model that was trained on the X_train data and apply it to the X_test
pred_lr = lr.predict(X_test)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_lr]))

#display confusion matrix for testset
lr_cm = confusion_matrix(y_test,pred_lr)
sns.heatmap(lr_cm,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = lr_cm[0]
true_pos, false_neg = lr_cm[1]

#print the accuracy score, recall, precision and F1 score
print("            Trainset             Testset")
print("Accuracy:  ",accuracy_score(y_train,pred_lr_train),"\t\t",accuracy_score(y_test,pred_lr))
print("Recall:    ",recall_score(y_train,pred_lr_train),"\t",recall_score(y_test,pred_lr))
print("Precision: ",precision_score(y_train,pred_lr_train),"\t",precision_score(y_test,pred_lr))
print("F1 score:  ",f1_score(y_train,pred_lr_train),"\t",f1_score(y_test,pred_lr))

from sklearn.metrics import roc_curve , auc

fpr, tpr, _ = roc_curve(y_test, pred_lr)
roc_auc = auc(fpr,tpr)

#Now Draw ROC using fpr , tpr
plt.plot([0, 1], [0, 1], 'k--',label = 'Random')
plt.plot(fpr,tpr,label ='ROC curve (area = %0.2f)' %roc_auc)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('Logistic Regression ROC curve')
plt.legend(loc ='best')

"""###Random Forest###"""

# Import the model
from sklearn.ensemble import RandomForestClassifier

# Instantiate model with 1000 decision trees
rf = RandomForestClassifier(n_estimators = 100)

# Train the model on training data
rf.fit(X_train,y_train)

#Take the model that was trained on the X_train data and apply it to the X_test
pred_rf_train = rf.predict(X_train)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_rf_train]))

#display confusion matrix for trainset
rf_cm_train = confusion_matrix(y_train,pred_rf_train)
sns.heatmap(rf_cm_train,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = rf_cm_train[0]
true_pos, false_neg = rf_cm_train[1]

#Take the model that was trained on the X_train data and apply it to the X_test
pred_rf = rf.predict(X_test)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_rf]))

#display confusion matrix
rf_cm = confusion_matrix(y_test,pred_rf)
sns.heatmap(rf_cm,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = rf_cm[0]
true_pos, false_neg = rf_cm[1]

#print the accuracy score, recall, precision and F1 score
print("            Trainset     Testset")
print("Accuracy:  ",accuracy_score(y_train,pred_rf_train),"\t",accuracy_score(y_test,pred_rf))
print("Recall:    ",recall_score(y_train,pred_rf_train),"\t",recall_score(y_test,pred_rf))
print("Precision: ",precision_score(y_train,pred_rf_train),"\t",precision_score(y_test,pred_rf))
print("F1 score:  ",f1_score(y_train,pred_rf_train),"\t",f1_score(y_test,pred_rf))

from sklearn.metrics import roc_curve , auc

fpr, tpr, _ = roc_curve(y_test, pred_rf)
roc_auc = auc(fpr,tpr)

#Now Draw ROC using fpr , tpr
plt.plot([0, 1], [0, 1], 'k--',label = 'Random')
plt.plot(fpr,tpr,label ='ROC curve (area = %0.2f)' %roc_auc)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('Random Forest ROC curve')
plt.legend(loc ='best')

"""####Feature importance####"""

rf_feat_importances = pd.Series(rf.feature_importances_, index=C.columns)
rf_feat_importances.nlargest(10).plot(kind='barh')

"""### Decision Tree###"""

#import decision tree classifier
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
#train using the algorithm
dt.fit(X_train,y_train)
pred_dt_train = dt.predict(X_train)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_dt_train]))

#display confusion matrix for trainset
dt_cm_train = confusion_matrix(y_train,pred_dt_train)
sns.heatmap(dt_cm_train,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = dt_cm_train[0]
true_pos, false_neg = dt_cm_train[1]

#use trainset to do prediction on testset
pred_dt = dt.predict(X_test)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_dt]))

#display confusion matrix for testset
dt_cm=confusion_matrix(y_test,pred_dt)
sns.heatmap(dt_cm,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = dt_cm[0]
true_pos, false_neg = dt_cm[1]

#print the accuracy score, recall, precision and F1 score
print("            Trainset             Testset")
print("Accuracy:  ",accuracy_score(y_train,pred_dt_train),"\t",accuracy_score(y_test,pred_dt))
print("Recall:    ",recall_score(y_train,pred_dt_train),"\t",recall_score(y_test,pred_dt))
print("Precision: ",precision_score(y_train,pred_dt_train),"\t",precision_score(y_test,pred_dt))
print("F1 score:  ",f1_score(y_train,pred_dt_train),"\t",f1_score(y_test,pred_dt))

from sklearn.metrics import roc_curve , auc

fpr, tpr, _ = roc_curve(y_test, pred_dt)
roc_auc = auc(fpr,tpr)

#Now Draw ROC using fpr , tpr
plt.plot([0, 1], [0, 1], 'k--',label = 'Random')
plt.plot(fpr,tpr,label ='ROC curve (area = %0.2f)' %roc_auc)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('Decision Tree ROC curve (test)')
plt.legend(loc ='best')

# feature importance
print(dt.feature_importances_)
# plot
dt_feat_importances = pd.Series(dt.feature_importances_, index=C.columns)
dt_feat_importances.nlargest(10).plot(kind='barh')

"""###XGBoost Classifier###"""

from xgboost import XGBClassifier
from mlxtend.plotting import plot_confusion_matrix

xgb = XGBClassifier(max_depth = 7,random_state=0, n_estimators=100, eval_metric = 'auc', min_child_weight = 1,
                    colsample_bylevel = 1, subsample= 1)
xgb.fit(X_train, y_train)

#Take the model that was trained on the X_train data and apply it to the X_test
pred_xgb = xgb.predict(X_test)
pred_xgb_train = xgb.predict(X_train)

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_xgb_train]))

#display confusion matrix
xgb_cm = confusion_matrix(y_train,pred_xgb_train)
sns.heatmap(xgb_cm,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = xgb_cm[0]
true_pos, false_neg = xgb_cm[1]

options = {0: "No", 1: "Yes"}
print(np.array([options[x] for x in pred_xgb]))

#display confusion matrix
xgb_cm = confusion_matrix(y_test,pred_xgb)
sns.heatmap(xgb_cm,xticklabels=['predicted_No', 'predicted_Yes'], yticklabels=['actual_No','actual_Yes'],
           annot=True, fmt='d', annot_kws={'fontsize':20},cmap="YlGnBu");
true_neg, false_pos = xgb_cm[0]
true_pos, false_neg = xgb_cm[1]

#accuracy_xgb_test = accuracy_score(y_test,pred_xgb)
#recall_xgb_test = recall_score(y_test,pred_xgb)
#precision_xgb_test = precision_score(y_test,pred_xgb)
#f1score_xgb_test = f1_score(y_test,pred_xgb)

#print the accuracy score, recall, precision and F1 score
print("            Trainset             Testset")
print("Accuracy:  ",accuracy_score(y_train,pred_xgb_train),"\t",accuracy_score(y_test,pred_xgb))
print("Recall:    ",recall_score(y_train,pred_xgb_train),"\t",recall_score(y_test,pred_xgb))
print("Precision: ",precision_score(y_train,pred_xgb_train),"\t",precision_score(y_test,pred_xgb))
print("F1 score:  ",f1_score(y_train,pred_xgb_train),"\t",f1_score(y_test,pred_xgb))

from sklearn.metrics import roc_curve , auc

fpr, tpr, _ = roc_curve(y_test, pred_xgb)
roc_auc = auc(fpr,tpr)

#Now Draw ROC using fpr , tpr
plt.plot([0, 1], [0, 1], 'k--',label = 'Random')
plt.plot(fpr,tpr,label ='ROC curve (area = %0.2f)' %roc_auc)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('XGBoost ROC curve (test)')
plt.legend(loc ='best')

"""#### Feature Importance"""

# feature importance
print(xgb.feature_importances_)
# plot
xgb_feat_importances = pd.Series(xgb.feature_importances_, index=C.columns)
xgb_feat_importances.nlargest(10).plot(kind='barh')

"""## **E. Evaluation**##

###Accuracy###
Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. The highest accuracy score in the models is XGBoost Classifier model, the accuracy approximate to the 0.8625 which means 86.25% accurate.
<table style='border:1px solid black'>
  <tr style='font-size: 14px;'>
      <th style='text-align: center;border: 1px solid black;'>Models</th>
      <th style='text-align: center;border: 1px solid black;'>Accuracy Score</th>
  </tr>

  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Logistic Regression</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.6685</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Random Forest</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.8580</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Decision Tree </td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.7995</td>
  </tr>
    <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>XGBoost Classifier</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.8625</td>
  </tr>
</table>

###Precision###
Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The highest precision score in the models is XGBoost Classifier model, the precision approximate to the 0.7019 which is pretty good. High precision relates to the low false positive rate.
<table style='border:1px solid black'>
  <tr style='font-size: 14px;'>
      <th style='text-align: center;border: 1px solid black;'>Models</th>
      <th style='text-align: center;border: 1px solid black;'>Precision Score</th>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Logistic Regression</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.3468</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Random Forest</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.6897</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Decision Tree </td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.5044</td>
  </tr>
    <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>XGBoost Classifier</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.7019</td>
  </tr>
</table>

###Recall###
Recall is the ratio of correctly predicted positive observations to the all observations in actual class. The highest recall score in the model is SVM model, the recall score which is approximate to the 0.7654.
<table style='border:1px solid black'>
  <tr style='font-size: 14px;'>
      <th style='text-align: center;border: 1px solid black;'>Models</th>
      <th style='text-align: center;border: 1px solid black;'>Recall Score</th>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Logistic Regression</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.7210</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Random Forest</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.5432</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Decision Tree </td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.5704</td>
  </tr>
    <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>XGBoost Classifier</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.5580</td>
  </tr>
</table>

###F1 Score###
F1 Score is the weighted average of Precision and Recall.This score takes both false positives and false negatives into account. The highest F1 score in the models is XGBoost Classifier model, the F1 score approximate to the 0.6217.
<table style='border:1px solid black'>
  <tr style='font-size: 14px;'>
      <th style='text-align: center;border: 1px solid black;'>Models</th>
      <th style='text-align: center;border: 1px solid black;'>F1 Score</th>
  </tr>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Logistic Regression</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.4683</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Random Forest</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.6077</td>
  </tr>
  <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>Decision Tree </td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.5353</td>
  </tr>
    <tr style='font-size: 14px;'>
      <td style='text-align: left;border: 1px solid black;'>XGBoost Classifier</td>
      <td style='text-align: left;border: 1px solid black;'>approximate 0.6217</td>
  </tr>
</table>
"""

import itertools

evaluation_lst = [lr, rf, dt, xgb]
evaluation_label = [ 'Logistic Regression', 'Random Forest',  'Decision Tree', 'XGBoost', ]

eva_fig = plt.figure(figsize=(13, 15))
eva_fig.set_facecolor("#F3F3F3")

for i, k in zip(evaluation_lst, evaluation_label):
    plt.subplot(4, 3, evaluation_lst.index(i) + 1)
    eva_predictions = i.predict(X_test)
    conf_matrix = confusion_matrix(y_test, eva_predictions)
    sns.heatmap(conf_matrix, annot=True, fmt="d", square=True,
                xticklabels=['predicted_No', 'predicted_Yes'],
                yticklabels=['actual_No', 'actual_Yes'],
                linewidths=2, linecolor="w", cmap="Set1")
    plt.title(k, color="b")
    plt.subplots_adjust(wspace=0.3, hspace=0.3)

plt.show()

"""## **F. Deployment**##

###Model Performance Metrics###

As a result, XGBoost classifier was chosen as a best model among these model listed. This is because xgboost classifier has the highest accuracy, precision and f1 scores among the models. However, the most important evaluation element to decide which is the best model is precision.

After that, we will build the XGBoost classifier model by using Flask which is an application enable us to create a basic server application to host the XGBoost classifier model (Vonage Developer Blog, 2020). At first, we need to load the XGBoost classifier model when the server starts and create an Application Programming Interface (API) endpoint called predict(). This endpoint will get the input variables and transform the categorical data into numeric data in order to make customer churn prediction. Then, this endpoint will generate the customers' data, invoke the model used and return the churn prediction. As a result, we can do churn prediction by getting new inputs to the endpoint.
"""

#gives model report in dataframe
def model_report(testing_y,pred,name) :
    accuracy     = accuracy_score(testing_y,pred)
    recallscore  = recall_score(testing_y,pred)
    precision    = precision_score(testing_y,pred)
    f1score      = f1_score(testing_y,pred)

    df = pd.DataFrame({"Model"           : [name],
                       "Accuracy_score"  : [accuracy],
                       "Recall_score"    : [recallscore],
                       "Precision"       : [precision],
                       "f1_score"        : [f1score],
                      })
    return df

#outputs for every model
model_lr = model_report(y_test,pred_lr,"Logistic Regression")
model_rf = model_report(y_test,pred_rf,"Random Forest")
model_dt = model_report(y_test,pred_dt,"Decision Tree")
model_xgb = model_report(y_test,pred_xgb,"XGBoost Classifier")


model_performances = pd.concat([model_lr,model_rf,model_dt,model_xgb,],axis = 0).reset_index()

model_performances = model_performances.drop(columns = "index",axis =1)
model_performances

#Grid Search for the best model which is XGBoost classifier
from sklearn.model_selection import GridSearchCV
clf = XGBClassifier(C=5,probability = True)
grid_values = parameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 1],
          'min_samples_split':[2,5,10,100],
          'max_depth':[2,3,5,50]}
grid_clf_acc = GridSearchCV(clf, param_grid = grid_values)
grid_clf_acc.fit(X_train, y_train)

#Predict values based on new parameters
y_pred_acc = grid_clf_acc.predict(X_test)

# New Model Evaluation metrics
print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))
print('Precision Score : ' + str(precision_score(y_test,y_pred_acc)))
print('Recall Score : ' + str(recall_score(y_test,y_pred_acc)))
print('F1 Score : ' + str(f1_score(y_test,y_pred_acc)))

#Logistic Regression (Grid Search) Confusion matrix
confusion_matrix(y_test,y_pred_acc)

"""## **G. Conclusion**##

###Result###

As a result, the XGBoost algorithm has the highest accuracy which is 0.8645.  Random Forest and LightGBM Classifier also have a high accuracy of 0.8580 and 0.8620 respectively compared to other models. These three algorithms also have similar scores for recall, precision and f1. But XGBoost is a little higher for all of the score, recall score : 0.560493, precision score : 0.709375, f1 score : 0.626206 so it's the reason it was chosen in implementation.

###Overfitting Test###
In order to check if the model is overfitting or not, KFold Cross Validation(CV) is used to solve this problem by dividing the data into folds and ensuring that each fold is used as a testing set at some point. It found out when using XGBoost Classifier the accuracy is 86.280 % considered as high accuracy and standard deviation is 0.552%. As a conclusion, model XGBoost used is not overfitting.

###Limitation###

The dataset used is considered less, so the accuracy may be affected if the dataset is large. Next, there are still many algorithms available but only 9 of them are chosen to be tested so maybe still have other algorithms that are better than XGBoost.
"""

from sklearn import model_selection

#X = bank.drop("Exited", axis=1)
#y = bank['Exited']

modelChosen = XGBClassifier()
kfold = model_selection.KFold(n_splits=5,random_state=1,shuffle = True)
result = model_selection.cross_val_score(modelChosen, X, y, cv=kfold)

print("Accuracy : %.3f%% " % (result.mean()*100.0))
print("Standard Deviation : %.3f%% " % (result.std()*100.0))